{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEBRA Best Practices with üêü<b>i</b><span style=\"color: #3C92ED;\">Tuna</span>\n",
    "\n",
    "*This notebook is based on the [\"Best Practices for Training CEBRA models\" notebook](https://cebra.ai/docs/demo_notebooks/CEBRA_best_practices.html)*\n",
    "\n",
    "This demo shows a complete workflow for training CEBRA models with consistency evaluation using iTuna. We cover:\n",
    "\n",
    "1. Setting up a CEBRA model\n",
    "2. Loading neural data\n",
    "3. Train/validation splits\n",
    "4. Consistency evaluation with `ConsistencyEnsemble`\n",
    "5. Visualizing and interpreting results\n",
    "6. Grid search for hyperparameters\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install ituna cebra[datasets,integrations]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cebra import CEBRA\n",
    "import cebra.datasets\n",
    "\n",
    "import ituna\n",
    "from ituna import ConsistencyEnsemble, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up a CEBRA Model\n",
    "\n",
    "CEBRA is a self-supervised representation learning method for neural data. It learns embeddings that capture the temporal structure of neural activity.\n",
    "\n",
    "CEBRA models are identifiable up to an **affine transformation**, so we use `metrics.Linear()` (which includes the intercept) as our indeterminacy class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CEBRA-Time model\n",
    "cebra_model = CEBRA(\n",
    "    model_architecture=\"offset10-model\",\n",
    "    batch_size=512,\n",
    "    learning_rate=3e-4,\n",
    "    temperature=1.12,\n",
    "    max_iterations=500,\n",
    "    conditional=\"time\",\n",
    "    output_dimension=3,\n",
    "    distance=\"cosine\",\n",
    "    device=\"cuda_if_available\",\n",
    "    verbose=True,\n",
    "    time_offsets=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Data\n",
    "\n",
    "We'll use the rat hippocampus dataset from CEBRA's built-in datasets. This contains neural recordings from hippocampus during spatial navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hippocampus dataset\n",
    "hippocampus = cebra.datasets.init(\"rat-hippocampus-single-achilles\")\n",
    "\n",
    "neural_data = hippocampus.neural.numpy()\n",
    "position_labels = hippocampus.continuous_index.numpy()\n",
    "\n",
    "print(f\"Neural data shape: {neural_data.shape}\")\n",
    "print(f\"Position labels shape: {position_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Train/Validation Split\n",
    "\n",
    "For proper evaluation, we split the data temporally into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split (80% train, 20% validation)\n",
    "split_idx = int(len(neural_data) * 0.8)\n",
    "\n",
    "train_data = neural_data[:split_idx]\n",
    "val_data = neural_data[split_idx:]\n",
    "\n",
    "train_labels = position_labels[:split_idx]\n",
    "val_labels = position_labels[split_idx:]\n",
    "\n",
    "print(f\"Train data: {train_data.shape}\")\n",
    "print(f\"Validation data: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit with ConsistencyEnsemble\n",
    "\n",
    "Now we wrap the CEBRA model in a `ConsistencyEnsemble` to train multiple instances and evaluate consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ConsistencyEnsemble with Linear indeterminacy (for CEBRA)\n",
    "ensemble = ConsistencyEnsemble(\n",
    "    estimator=cebra_model,\n",
    "    consistency_transform=metrics.PairwiseConsistency(\n",
    "        indeterminacy=metrics.Linear(),  # CEBRA is identifiable up to linear transform\n",
    "        symmetric=False,\n",
    "        include_diagonal=True,\n",
    "    ),\n",
    "    random_states=5,  # Train 5 models\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "ensemble.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate consistency\n",
    "train_score = ensemble.score(train_data)\n",
    "print(f\"Train consistency score: {train_score:.4f}\")\n",
    "\n",
    "# Also check on validation data\n",
    "val_score = ensemble.score(val_data)\n",
    "print(f\"Validation consistency score: {val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Embeddings\n",
    "\n",
    "Let's visualize the learned embeddings colored by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aligned embeddings\n",
    "train_embeddings = ensemble.transform(train_data)\n",
    "val_embeddings = ensemble.transform(val_data)\n",
    "\n",
    "print(f\"Train embedding shape: {train_embeddings.shape}\")\n",
    "print(f\"Validation embedding shape: {val_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3D embeddings\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Train embeddings\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "scatter1 = ax1.scatter(\n",
    "    train_embeddings[:, 0],\n",
    "    train_embeddings[:, 1],\n",
    "    train_embeddings[:, 2],\n",
    "    c=train_labels[:, 0],\n",
    "    cmap=\"rainbow\",\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax1.set_title(f\"Train (consistency: {train_score:.3f})\")\n",
    "ax1.set_xlabel(\"Dim 1\")\n",
    "ax1.set_ylabel(\"Dim 2\")\n",
    "ax1.set_zlabel(\"Dim 3\")\n",
    "\n",
    "# Validation embeddings\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "scatter2 = ax2.scatter(\n",
    "    val_embeddings[:, 0],\n",
    "    val_embeddings[:, 1],\n",
    "    val_embeddings[:, 2],\n",
    "    c=val_labels[:, 0],\n",
    "    cmap=\"rainbow\",\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    ")\n",
    "ax2.set_title(f\"Validation (consistency: {val_score:.3f})\")\n",
    "ax2.set_xlabel(\"Dim 1\")\n",
    "ax2.set_ylabel(\"Dim 2\")\n",
    "ax2.set_zlabel(\"Dim 3\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Pairwise Consistency\n",
    "\n",
    "We can examine the consistency between individual model pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed pairwise scores\n",
    "pairs, scores = train_embeddings.scores\n",
    "\n",
    "print(\"Pairwise consistency scores:\")\n",
    "for (i, j), score in zip(pairs, scores):\n",
    "    print(f\"  Model {i} -> Model {j}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean pairwise score: {np.mean(scores):.4f}\")\n",
    "print(f\"Std pairwise score: {np.std(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Grid Search with Consistency\n",
    "\n",
    "We can use sklearn's `GridSearchCV` with iTuna to find hyperparameters that yield consistent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"estimator__temperature\": [0.5, 1.0, 1.5],\n",
    "    \"estimator__output_dimension\": [3, 8],\n",
    "}\n",
    "\n",
    "# Create base ensemble\n",
    "base_ensemble = ConsistencyEnsemble(\n",
    "    estimator=CEBRA(\n",
    "        model_architecture=\"offset10-model\",\n",
    "        batch_size=512,\n",
    "        learning_rate=3e-4,\n",
    "        max_iterations=200,  # Fewer iterations for grid search\n",
    "        conditional=\"time\",\n",
    "        distance=\"cosine\",\n",
    "        device=\"cuda_if_available\",\n",
    "        verbose=False,\n",
    "        time_offsets=10,\n",
    "    ),\n",
    "    consistency_transform=metrics.PairwiseConsistency(\n",
    "        indeterminacy=metrics.Linear(),\n",
    "        symmetric=False,\n",
    "    ),\n",
    "    random_states=3,\n",
    ")\n",
    "\n",
    "# Run grid search\n",
    "# Note: This uses consistency score as the optimization target\n",
    "grid_search = GridSearchCV(\n",
    "    base_ensemble,\n",
    "    param_grid,\n",
    "    cv=2,\n",
    "    scoring=\"r2\",  # ConsistencyEnsemble.score() returns R2\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "# Fit (this will take a while)\n",
    "# grid_search.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment after running grid search:\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best consistency score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Backends for Large Experiments\n",
    "\n",
    "For large-scale experiments with many hyperparameters, use iTuna's caching backends to avoid re-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable disk caching for grid search\n",
    "with ituna.config.config_context(DEFAULT_BACKEND=\"disk_cache\"):\n",
    "    # Models will be cached, so re-running is fast\n",
    "    ensemble.fit(train_data)\n",
    "    print(f\"Consistency: {ensemble.score(train_data):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways for CEBRA with iTuna:\n",
    "\n",
    "1. **Use `metrics.Linear()` for CEBRA** - CEBRA embeddings are identifiable up to linear transformations\n",
    "2. **Train multiple seeds** - Use `random_states=5` or more for robust consistency estimates\n",
    "3. **Check both train and validation** - High consistency on both suggests stable representations\n",
    "4. **Use caching for grid search** - Enable `disk_cache` backend to avoid re-training\n",
    "5. **Consistency score > 0.9** - Generally indicates reliable, reproducible embeddings\n",
    "\n",
    "For more examples, see:\n",
    "- `ituna-experiments/cebra/` - Extended CEBRA experiments\n",
    "- `iTune Reference.ipynb` - Comprehensive reference notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
